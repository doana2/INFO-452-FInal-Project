{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 2: Component 1 - Fine-Tuned Sentiment Model\n",
        "## Building the DistilBERT Review Classifier\n",
        "\n",
        "**Focus:** Develop sentiment analysis model for customer reviews\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "# AI Assistance: Claude helped structure this installation cell\n",
        "\n",
        "!pip install -q transformers datasets torch accelerate scikit-learn\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Amazon review dataset\n",
        "# Using a small subset for quick training (1000 samples)\n",
        "# Citation: Amazon Polarity dataset from Hugging Face\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "# Load 1000 training samples and 200 test samples\n",
        "train_dataset = load_dataset(\"amazon_polarity\", split=\"train[:1000]\")\n",
        "test_dataset = load_dataset(\"amazon_polarity\", split=\"test[:200]\")\n",
        "\n",
        "print(f\"✅ Training samples: {len(train_dataset)}\")\n",
        "print(f\"✅ Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# Preview a sample\n",
        "sample = train_dataset[0]\n",
        "print(f\"\\nSample Review:\")\n",
        "print(f\"Label: {sample['label']} (0=Negative, 1=Positive)\")\n",
        "print(f\"Title: {sample['title']}\")\n",
        "print(f\"Content: {sample['content'][:150]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine title and content for better context\n",
        "# AI Assistance: Claude suggested combining fields for richer input\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Combines review title and content into single text field.\n",
        "    This gives the model more context for accurate sentiment prediction.\n",
        "    \"\"\"\n",
        "    # Combine title and content with separator\n",
        "    texts = []\n",
        "    for title, content in zip(examples['title'], examples['content']):\n",
        "        # Limit content length to avoid extremely long inputs\n",
        "        text = f\"{title}. {content[:300]}\"\n",
        "        texts.append(text)\n",
        "    return {'text': texts}\n",
        "\n",
        "# Apply preprocessing\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "print(\"✅ Dataset preprocessing complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Pre-trained Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load DistilBERT tokenizer and model\n",
        "# Model: distilbert-base-uncased from Hugging Face\n",
        "# DistilBERT is a smaller, faster version of BERT (40% smaller, 60% faster)\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "print(f\"Loading tokenizer and model: {model_name}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model for binary classification (positive/negative)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2  # Binary classification\n",
        ")\n",
        "\n",
        "print(\"✅ Model and tokenizer loaded successfully\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Tokenize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the text data\n",
        "# This converts text into numerical tokens that the model can process\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes text using DistilBERT tokenizer.\n",
        "    \n",
        "    Parameters:\n",
        "    - padding: Pads shorter sequences to match longest in batch\n",
        "    - truncation: Cuts off text longer than max_length\n",
        "    - max_length: Maximum token length (512 is BERT's limit)\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128  # Using 128 for faster training\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "tokenized_train.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_test.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(\"✅ Tokenization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Define Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define metrics for model evaluation\n",
        "# AI Assistance: Claude provided this metrics computation function\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, precision, recall, and F1-score.\n",
        "    These metrics help us understand model performance:\n",
        "    - Accuracy: Overall correctness\n",
        "    - Precision: How many predicted positives are actually positive\n",
        "    - Recall: How many actual positives we correctly identified\n",
        "    - F1: Harmonic mean of precision and recall\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='binary'\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "print(\"✅ Evaluation metrics defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Configure Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up training configuration\n",
        "# These parameters control how the model learns\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Where to save model checkpoints\n",
        "    num_train_epochs=3,               # Number of times to go through dataset\n",
        "    per_device_train_batch_size=16,   # Batch size for training\n",
        "    per_device_eval_batch_size=32,    # Batch size for evaluation\n",
        "    warmup_steps=100,                 # Gradual learning rate warmup\n",
        "    weight_decay=0.01,                # Regularization to prevent overfitting\n",
        "    logging_dir='./logs',             # Where to save training logs\n",
        "    logging_steps=50,                 # Log every 50 steps\n",
        "    eval_strategy=\"epoch\",         # Evaluate after each epoch\n",
        "    save_strategy=\"epoch\",            # Save model after each epoch\n",
        "    load_best_model_at_end=True,      # Load best model when done\n",
        "    push_to_hub=False                 # Don't upload to Hugging Face Hub yet\n",
        ")\n",
        "\n",
        "print(\"✅ Training configuration set\")\n",
        "print(f\"Training for {training_args.num_train_epochs} epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Initialize Trainer and Fine-Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Trainer instance\n",
        "# The Trainer handles the training loop automatically\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"✅ Trainer initialized\")\n",
        "print(\"Starting training... (this may take 5-10 minutes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "# This is transfer learning - we're adapting a pre-trained model to our task\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n✅ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n=== Model Performance ===\")\n",
        "print(f\"Accuracy:  {results['eval_accuracy']:.4f} ({results['eval_accuracy']*100:.2f}%)\")\n",
        "print(f\"Precision: {results['eval_precision']:.4f}\")\n",
        "print(f\"Recall:    {results['eval_recall']:.4f}\")\n",
        "print(f\"F1-Score:  {results['eval_f1']:.4f}\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "if results['eval_accuracy'] > 0.85:\n",
        "    print(\"✅ Excellent performance! Model is ready for deployment.\")\n",
        "elif results['eval_accuracy'] > 0.75:\n",
        "    print(\"✅ Good performance! Model is functional for demo purposes.\")\n",
        "else:\n",
        "    print(\"⚠️ Model works but could be improved with more training data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Test Model with Sample Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with custom reviews\n",
        "# AI Assistance: Claude helped create this inference function\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"\n",
        "    Predicts sentiment of a given review.\n",
        "    \n",
        "    Returns:\n",
        "    - label: 'Positive' or 'Negative'\n",
        "    - confidence: Probability score (0-1)\n",
        "    \"\"\"\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    \n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        \n",
        "    # Get predicted class and confidence\n",
        "    predicted_class = predictions.argmax().item()\n",
        "    confidence = predictions[0][predicted_class].item()\n",
        "    \n",
        "    label = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
        "    \n",
        "    return label, confidence\n",
        "\n",
        "# Test samples\n",
        "test_reviews = [\n",
        "    \"These headphones are amazing! Great sound quality and very comfortable.\",\n",
        "    \"Terrible product. Broke after one week. Don't waste your money.\",\n",
        "    \"It's okay, nothing special but does the job.\",\n",
        "    \"Best purchase ever! Highly recommend to everyone.\",\n",
        "    \"Poor quality, disappointed with this purchase.\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== Testing Model on Sample Reviews ===\")\n",
        "for i, review in enumerate(test_reviews, 1):\n",
        "    label, confidence = predict_sentiment(review)\n",
        "    print(f\"\\n{i}. Review: {review}\")\n",
        "    print(f\"   Prediction: {label} (Confidence: {confidence:.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Save Model for Later Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model and tokenizer\n",
        "# This allows us to load it later without retraining\n",
        "\n",
        "save_directory = \"./sentiment_model\"\n",
        "\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"✅ Model saved to {save_directory}\")\n",
        "print(\"You can load this model later with:\")\n",
        "print(f\"  model = AutoModelForSequenceClassification.from_pretrained('{save_directory}')\")\n",
        "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{save_directory}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Week 2 Summary\n",
        "\n",
        "**Completed:**\n",
        "- ✅ Loaded Amazon review dataset (1,000 training samples)\n",
        "- ✅ Fine-tuned DistilBERT for sentiment classification\n",
        "- ✅ Achieved working model with evaluation metrics\n",
        "- ✅ Tested model on sample reviews\n",
        "- ✅ Saved model for integration with GUI\n",
        "\n",
        "**Performance:**\n",
        "- Expected accuracy: ~85-92% (typical for this task)\n",
        "- Model can distinguish positive from negative reviews\n",
        "- Ready for integration with RAG system\n",
        "\n",
        "**Next Steps (Week 3-4):**\n",
        "- Build RAG system with ChromaDB\n",
        "- Create embeddings from company documents\n",
        "- Test question-answering capability\n",
        "\n",
        "---\n",
        "\n",
        "**AI Assistance Documentation:**\n",
        "- Claude (Anthropic) provided:\n",
        "  - Code structure and training pipeline\n",
        "  - Comments and explanations\n",
        "  - Evaluation metrics implementation\n",
        "  - Testing framework\n",
        "\n",
        "**Citations:**\n",
        "- Dataset: Zhang et al. (2015) - Amazon Review Dataset (Hugging Face: amazon_polarity)\n",
        "- Model: Sanh et al. (2019) - DistilBERT (Hugging Face: distilbert-base-uncased)\n",
        "- Framework: Hugging Face Transformers library"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
